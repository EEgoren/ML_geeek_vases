{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076a662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Loading tasks + building df...\n",
      "Occluding hair labels: ['Phrygian cap with lappets', 'ampyx', 'cap', 'opisthosphendone', 'petasos', 'pilos', 'saccos']\n",
      "Missing images: 0\n",
      "Total figure bboxes: 520\n",
      "Gender counts:\n",
      " gender\n",
      "male      359\n",
      "female    161\n",
      "Name: count, dtype: int64\n",
      "Occluding hair counts:\n",
      " occluding_hair\n",
      "0    476\n",
      "1     44\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== SEED 42 ===\n",
      "Train: 425 Val: 95\n",
      "Val occ counts:\n",
      " occluding_hair\n",
      "0    85\n",
      "1    10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.7210 | acc=0.642 | bal_acc=0.669 | f1m=0.635 | fem_rec=0.750 | occ_acc=0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.4182 | acc=0.747 | bal_acc=0.748 | f1m=0.732 | fem_rec=0.750 | occ_acc=0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.2861 | acc=0.789 | bal_acc=0.734 | f1m=0.747 | fem_rec=0.562 | occ_acc=0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.2469 | acc=0.811 | bal_acc=0.719 | f1m=0.742 | fem_rec=0.438 | occ_acc=0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.1502 | acc=0.758 | bal_acc=0.748 | f1m=0.738 | fem_rec=0.719 | occ_acc=0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.1027 | acc=0.758 | bal_acc=0.756 | f1m=0.741 | fem_rec=0.750 | occ_acc=0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | loss=0.1375 | acc=0.789 | bal_acc=0.718 | f1m=0.735 | fem_rec=0.500 | occ_acc=0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.1622 | acc=0.779 | bal_acc=0.695 | f1m=0.711 | fem_rec=0.438 | occ_acc=0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 42 | BEST_EPOCH 3 | macroF1=0.747\n",
      "\n",
      "=== NON-OCCLUDING hair (occ=0) (n=85) ===\n",
      "accuracy: 0.776\n",
      "balanced_accuracy: 0.669\n",
      "macro_f1: 0.684\n",
      "confusion:\n",
      "[[10 13]\n",
      " [ 6 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.625     0.435     0.513        23\n",
      "        male      0.812     0.903     0.855        62\n",
      "\n",
      "    accuracy                          0.776        85\n",
      "   macro avg      0.718     0.669     0.684        85\n",
      "weighted avg      0.761     0.776     0.762        85\n",
      "\n",
      "\n",
      "=== OCCLUDING hair (occ=1) (n=10) ===\n",
      "accuracy: 0.900\n",
      "balanced_accuracy: 0.944\n",
      "macro_f1: 0.804\n",
      "confusion:\n",
      "[[8 1]\n",
      " [0 1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      1.000     0.889     0.941         9\n",
      "        male      0.500     1.000     0.667         1\n",
      "\n",
      "    accuracy                          0.900        10\n",
      "   macro avg      0.750     0.944     0.804        10\n",
      "weighted avg      0.950     0.900     0.914        10\n",
      "\n",
      "\n",
      "=== SEED 43 ===\n",
      "Train: 437 Val: 83\n",
      "Val occ counts:\n",
      " occluding_hair\n",
      "0    76\n",
      "1     7\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.7245 | acc=0.735 | bal_acc=0.774 | f1m=0.730 | fem_rec=0.893 | occ_acc=0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.4902 | acc=0.675 | bal_acc=0.737 | f1m=0.674 | fem_rec=0.929 | occ_acc=0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.3320 | acc=0.699 | bal_acc=0.755 | f1m=0.697 | fem_rec=0.929 | occ_acc=0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.1911 | acc=0.831 | bal_acc=0.820 | f1m=0.814 | fem_rec=0.786 | occ_acc=0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.1370 | acc=0.819 | bal_acc=0.811 | f1m=0.803 | fem_rec=0.786 | occ_acc=0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.0936 | acc=0.819 | bal_acc=0.837 | f1m=0.810 | fem_rec=0.893 | occ_acc=0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | loss=0.1269 | acc=0.807 | bal_acc=0.828 | f1m=0.799 | fem_rec=0.893 | occ_acc=0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.1151 | acc=0.795 | bal_acc=0.802 | f1m=0.783 | fem_rec=0.821 | occ_acc=0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | loss=0.0796 | acc=0.819 | bal_acc=0.776 | f1m=0.788 | fem_rec=0.643 | occ_acc=0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 43 | BEST_EPOCH 4 | macroF1=0.814\n",
      "\n",
      "=== NON-OCCLUDING hair (occ=0) (n=76) ===\n",
      "accuracy: 0.829\n",
      "balanced_accuracy: 0.808\n",
      "macro_f1: 0.804\n",
      "confusion:\n",
      "[[18  6]\n",
      " [ 7 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.720     0.750     0.735        24\n",
      "        male      0.882     0.865     0.874        52\n",
      "\n",
      "    accuracy                          0.829        76\n",
      "   macro avg      0.801     0.808     0.804        76\n",
      "weighted avg      0.831     0.829     0.830        76\n",
      "\n",
      "\n",
      "=== OCCLUDING hair (occ=1) (n=7) ===\n",
      "accuracy: 0.857\n",
      "balanced_accuracy: 0.833\n",
      "macro_f1: 0.844\n",
      "confusion:\n",
      "[[4 0]\n",
      " [1 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.800     1.000     0.889         4\n",
      "        male      1.000     0.667     0.800         3\n",
      "\n",
      "    accuracy                          0.857         7\n",
      "   macro avg      0.900     0.833     0.844         7\n",
      "weighted avg      0.886     0.857     0.851         7\n",
      "\n",
      "\n",
      "=== SEED 44 ===\n",
      "Train: 409 Val: 111\n",
      "Val occ counts:\n",
      " occluding_hair\n",
      "0    103\n",
      "1      8\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.6223 | acc=0.685 | bal_acc=0.700 | f1m=0.656 | fem_rec=0.733 | occ_acc=0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.4071 | acc=0.658 | bal_acc=0.713 | f1m=0.642 | fem_rec=0.833 | occ_acc=0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.2851 | acc=0.730 | bal_acc=0.699 | f1m=0.682 | fem_rec=0.633 | occ_acc=0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.2262 | acc=0.739 | bal_acc=0.727 | f1m=0.700 | fem_rec=0.700 | occ_acc=0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.1501 | acc=0.721 | bal_acc=0.725 | f1m=0.688 | fem_rec=0.733 | occ_acc=0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.1120 | acc=0.766 | bal_acc=0.745 | f1m=0.724 | fem_rec=0.700 | occ_acc=0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | loss=0.0972 | acc=0.712 | bal_acc=0.719 | f1m=0.680 | fem_rec=0.733 | occ_acc=0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.0743 | acc=0.811 | bal_acc=0.744 | f1m=0.752 | fem_rec=0.600 | occ_acc=0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | loss=0.0699 | acc=0.712 | bal_acc=0.740 | f1m=0.687 | fem_rec=0.800 | occ_acc=0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | loss=0.0521 | acc=0.775 | bal_acc=0.762 | f1m=0.737 | fem_rec=0.733 | occ_acc=0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | loss=0.0884 | acc=0.793 | bal_acc=0.701 | f1m=0.715 | fem_rec=0.500 | occ_acc=0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | loss=0.0651 | acc=0.748 | bal_acc=0.712 | f1m=0.698 | fem_rec=0.633 | occ_acc=0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | loss=0.0853 | acc=0.595 | bal_acc=0.701 | f1m=0.591 | fem_rec=0.933 | occ_acc=0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 44 | BEST_EPOCH 8 | macroF1=0.752\n",
      "\n",
      "=== NON-OCCLUDING hair (occ=0) (n=103) ===\n",
      "accuracy: 0.816\n",
      "balanced_accuracy: 0.735\n",
      "macro_f1: 0.738\n",
      "confusion:\n",
      "[[14 10]\n",
      " [ 9 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.609     0.583     0.596        24\n",
      "        male      0.875     0.886     0.881        79\n",
      "\n",
      "    accuracy                          0.816       103\n",
      "   macro avg      0.742     0.735     0.738       103\n",
      "weighted avg      0.813     0.816     0.814       103\n",
      "\n",
      "\n",
      "=== OCCLUDING hair (occ=1) (n=8) ===\n",
      "accuracy: 0.750\n",
      "balanced_accuracy: 0.833\n",
      "macro_f1: 0.733\n",
      "confusion:\n",
      "[[4 2]\n",
      " [0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      1.000     0.667     0.800         6\n",
      "        male      0.500     1.000     0.667         2\n",
      "\n",
      "    accuracy                          0.750         8\n",
      "   macro avg      0.750     0.833     0.733         8\n",
      "weighted avg      0.875     0.750     0.767         8\n",
      "\n",
      "\n",
      "=== SUMMARY (3 seeds) ===\n",
      "   seed  best_epoch   val_acc  val_bal_acc  val_f1_macro  val_female_recall  \\\n",
      "0    42           3  0.789474     0.733631      0.746802           0.562500   \n",
      "1    43           4  0.831325     0.820130      0.814496           0.785714   \n",
      "2    44           8  0.810811     0.744444      0.752153           0.600000   \n",
      "\n",
      "   val_occ_acc  val_occ_n  val_total  \n",
      "0     0.905263         10         95  \n",
      "1     0.951807          7         83  \n",
      "2     0.891892          8        111  \n",
      "\n",
      "Averages:\n",
      "val_acc              0.810537\n",
      "val_bal_acc          0.766068\n",
      "val_f1_macro         0.771150\n",
      "val_female_recall    0.649405\n",
      "val_occ_acc          0.916321\n",
      "dtype: float64\n",
      "\n",
      "Std:\n",
      "val_acc              0.020927\n",
      "val_bal_acc          0.047130\n",
      "val_f1_macro         0.037633\n",
      "val_female_recall    0.119527\n",
      "val_occ_acc          0.031451\n",
      "dtype: float64\n",
      "\n",
      "Outputs in: C:\\Users\\Katya\\mag_vase\\ML\\runs_gender_Bpp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix, classification_report,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "JSON_PATH = Path(\"LS_export_26.12.json\")\n",
    "LABELS_CSV = Path(\"label_type_gender.csv\")\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "PAD = 0.0\n",
    "MIN_CROP_PX = 16 \n",
    "\n",
    "LAMBDA_OCC = 0.2\n",
    "\n",
    "RUNS_DIR = Path(\"runs_gender_Bpp\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEEDS = [42, 43, 44]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "#\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = (torch.initial_seed() + worker_id) % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "#\n",
    "\n",
    "def bbox_xywh_to_xyxy(b):\n",
    "    x, y, w, h = b\n",
    "    return (x, y, x + w, y + h)\n",
    "\n",
    "def inter_area_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    inter_x1 = max(ax1, bx1)\n",
    "    inter_y1 = max(ay1, by1)\n",
    "    inter_x2 = min(ax2, bx2)\n",
    "    inter_y2 = min(ay2, by2)\n",
    "    iw = max(0.0, inter_x2 - inter_x1)\n",
    "    ih = max(0.0, inter_y2 - inter_y1)\n",
    "    return iw * ih\n",
    "\n",
    "def area_xyxy(b):\n",
    "    x1, y1, x2, y2 = b\n",
    "    return max(0.0, x2 - x1) * max(0.0, y2 - y1)\n",
    "\n",
    "def ioh_xyxy(fig_xyxy, head_xyxy):\n",
    "\n",
    "    ah = area_xyxy(head_xyxy)\n",
    "    if ah <= 0:\n",
    "        return 0.0\n",
    "    inter = inter_area_xyxy(fig_xyxy, head_xyxy)\n",
    "    return inter / ah\n",
    "\n",
    "def head_region_overlap(fig_xyxy, head_xyxy, head_top_frac=0.45):\n",
    "    fx1, fy1, fx2, fy2 = fig_xyxy\n",
    "    hx1, hy1, hx2, hy2 = head_xyxy\n",
    "    head_limit = fy1 + (fy2 - fy1) * head_top_frac\n",
    "    cy = (hy1 + hy2) / 2.0\n",
    "    return cy <= head_limit\n",
    "\n",
    "#\n",
    "\n",
    "print(\"\\nLoading tasks + building df...\")\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "label_df = pd.read_csv(LABELS_CSV, encoding=\"utf-8\")\n",
    "label2gender = dict(zip(label_df[\"label\"], label_df[\"gender\"]))\n",
    "\n",
    "occ_labels = set(\n",
    "    label_df.loc[\n",
    "        label_df[\"type\"].astype(str).str.contains(\"attribute_occluding_hair\", case=False, na=False),\n",
    "        \"label\"\n",
    "    ].tolist()\n",
    ")\n",
    "print(\"Occluding hair labels:\", sorted(list(occ_labels)))\n",
    "\n",
    "rows = []\n",
    "missing_images = 0\n",
    "\n",
    "IOH_THR = 0.30\n",
    "REQUIRE_HEAD_REGION = True\n",
    "HEAD_TOP_FRAC = 0.45\n",
    "\n",
    "for task in tasks:\n",
    "    task_id = task.get(\"id\")\n",
    "    data = task.get(\"data\", {}) or {}\n",
    "    image_path = data.get(\"image_local_path\")\n",
    "    museum_number = data.get(\"Museum number\")\n",
    "\n",
    "    if not image_path:\n",
    "        missing_images += 1\n",
    "        continue\n",
    "\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.exists():\n",
    "        missing_images += 1\n",
    "        continue\n",
    "\n",
    "    figure_boxes = []\n",
    "    headwear_boxes = []\n",
    "\n",
    "    for ann in task.get(\"annotations\", []):\n",
    "        for r in ann.get(\"result\", []):\n",
    "            if r.get(\"type\") != \"rectanglelabels\":\n",
    "                continue\n",
    "            value = r.get(\"value\", {}) or {}\n",
    "            labels = value.get(\"rectanglelabels\", [])\n",
    "            if not labels:\n",
    "                continue\n",
    "\n",
    "            lbl = labels[0]\n",
    "            x = float(value[\"x\"])\n",
    "            y = float(value[\"y\"])\n",
    "            w = float(value[\"width\"])\n",
    "            h = float(value[\"height\"])\n",
    "            box_xyxy = bbox_xywh_to_xyxy((x, y, w, h))\n",
    "\n",
    "            if lbl in occ_labels:\n",
    "                headwear_boxes.append((lbl, box_xyxy))\n",
    "                continue\n",
    "\n",
    "            g = label2gender.get(lbl)\n",
    "            if g in {\"male\", \"female\"}:\n",
    "                figure_boxes.append((lbl, g, box_xyxy))\n",
    "\n",
    "    for fig_label, fig_gender, fig_xyxy in figure_boxes:\n",
    "        occ = 0\n",
    "        for head_label, head_xyxy in headwear_boxes:\n",
    "            if REQUIRE_HEAD_REGION and not head_region_overlap(fig_xyxy, head_xyxy, head_top_frac=HEAD_TOP_FRAC):\n",
    "                continue\n",
    "            if ioh_xyxy(fig_xyxy, head_xyxy) >= IOH_THR:\n",
    "                occ = 1\n",
    "                break\n",
    "\n",
    "        fx1, fy1, fx2, fy2 = fig_xyxy\n",
    "        rows.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"museum_number\": museum_number if museum_number is not None else str(task_id),\n",
    "            \"image_path\": str(image_path),\n",
    "            \"label\": fig_label,\n",
    "            \"gender\": fig_gender,\n",
    "            \"occluding_hair\": int(occ),\n",
    "            \"x\": float(fx1),\n",
    "            \"y\": float(fy1),\n",
    "            \"w\": float(fx2 - fx1),\n",
    "            \"h\": float(fy2 - fy1),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Missing images:\", missing_images)\n",
    "print(\"Total figure bboxes:\", len(df))\n",
    "print(\"Gender counts:\\n\", df[\"gender\"].value_counts())\n",
    "print(\"Occluding hair counts:\\n\", df[\"occluding_hair\"].value_counts())\n",
    "\n",
    "#\n",
    "\n",
    "class MultiTaskBboxDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, pad: float = 0.0):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.pad = pad\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.image_path).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "\n",
    "        x1 = row.x / 100.0 * W\n",
    "        y1 = row.y / 100.0 * H\n",
    "        x2 = (row.x + row.w) / 100.0 * W\n",
    "        y2 = (row.y + row.h) / 100.0 * H\n",
    "\n",
    "        if (x2 - x1) < MIN_CROP_PX or (y2 - y1) < MIN_CROP_PX:\n",
    "            raise ValueError(\n",
    "                f\"Too-small crop at idx={idx}: \"\n",
    "                f\"w={(x2-x1):.2f}px h={(y2-y1):.2f}px \"\n",
    "                f\"task_id={row.task_id} image={row.image_path}\"\n",
    "            )\n",
    "\n",
    "        if self.pad > 0:\n",
    "            pad_x = self.pad * (x2 - x1)\n",
    "            pad_y = self.pad * (y2 - y1)\n",
    "            x1 = max(0, x1 - pad_x)\n",
    "            y1 = max(0, y1 - pad_y)\n",
    "            x2 = min(W, x2 + pad_x)\n",
    "            y2 = min(H, y2 + pad_y)\n",
    "\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        y_gender = 1 if row.gender == \"male\" else 0\n",
    "        y_occ = float(row.occluding_hair)\n",
    "\n",
    "        meta = {\n",
    "            \"task_id\": row.task_id,\n",
    "            \"museum_number\": row.museum_number,\n",
    "            \"image_path\": row.image_path,\n",
    "            \"label\": row.label,\n",
    "            \"gender\": row.gender,\n",
    "            \"occluding_hair\": int(row.occluding_hair),\n",
    "            \"x\": float(row.x),\n",
    "            \"y\": float(row.y),\n",
    "            \"w\": float(row.w),\n",
    "            \"h\": float(row.h),\n",
    "        }\n",
    "\n",
    "        return crop, y_gender, y_occ, meta\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    xs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    yg = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    yo = torch.tensor([b[2] for b in batch], dtype=torch.float32)\n",
    "    metas = [b[3] for b in batch]\n",
    "    return xs, yg, yo, metas\n",
    "\n",
    "#\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomAffine(degrees=5, translate=(0.03, 0.03), scale=(0.95, 1.05)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#\n",
    "\n",
    "def save_error_crop(meta, pred_label, prob_male, out_dir):\n",
    "    img = Image.open(meta[\"image_path\"]).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "\n",
    "    x1 = meta[\"x\"] / 100.0 * W\n",
    "    y1 = meta[\"y\"] / 100.0 * H\n",
    "    x2 = (meta[\"x\"] + meta[\"w\"]) / 100.0 * W\n",
    "    y2 = (meta[\"y\"] + meta[\"h\"]) / 100.0 * H\n",
    "\n",
    "    if PAD > 0:\n",
    "        pad_x = PAD * (x2 - x1)\n",
    "        pad_y = PAD * (y2 - y1)\n",
    "        x1 = max(0, x1 - pad_x)\n",
    "        y1 = max(0, y1 - pad_y)\n",
    "        x2 = min(W, x2 + pad_x)\n",
    "        y2 = min(H, y2 + pad_y)\n",
    "\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    draw = ImageDraw.Draw(crop)\n",
    "    caption = (\n",
    "        f\"true={meta['gender']} pred={'male' if pred_label==1 else 'female'} \"\n",
    "        f\"p_male={prob_male:.2f} occ={meta['occluding_hair']}\"\n",
    "    )\n",
    "    draw.rectangle([0, 0, crop.size[0], 18], fill=(0, 0, 0))\n",
    "    draw.text((3, 2), caption, fill=(255, 255, 255))\n",
    "\n",
    "    fname = (\n",
    "        f\"task{meta['task_id']}_{Path(meta['image_path']).stem}_\"\n",
    "        f\"{meta['label']}_occ{meta['occluding_hair']}_\"\n",
    "        f\"x{meta['x']:.1f}_y{meta['y']:.1f}_w{meta['w']:.1f}_h{meta['h']:.1f}.jpg\"\n",
    "    )\n",
    "    crop.save(out_dir / fname)\n",
    "\n",
    "#\n",
    "\n",
    "def subgroup_metrics(y_true, y_pred, occ_flags, out_path_txt=None, title_prefix=\"\"):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    occ_flags = np.array(occ_flags)\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    def one(mask, name):\n",
    "        yt = y_true[mask]\n",
    "        yp = y_pred[mask]\n",
    "        if len(yt) == 0:\n",
    "            lines.append(f\"\\n=== {name}: no samples ===\\n\")\n",
    "            return\n",
    "        acc = accuracy_score(yt, yp)\n",
    "        bal = balanced_accuracy_score(yt, yp)\n",
    "        f1m = f1_score(yt, yp, average=\"macro\")\n",
    "        rep = classification_report(yt, yp, target_names=[\"female\", \"male\"], digits=3, zero_division=0)\n",
    "        cm = confusion_matrix(yt, yp, labels=[0, 1])\n",
    "\n",
    "        lines.append(f\"\\n=== {name} (n={len(yt)}) ===\")\n",
    "        lines.append(f\"accuracy: {acc:.3f}\")\n",
    "        lines.append(f\"balanced_accuracy: {bal:.3f}\")\n",
    "        lines.append(f\"macro_f1: {f1m:.3f}\")\n",
    "        lines.append(\"confusion:\\n\" + str(cm))\n",
    "        lines.append(rep)\n",
    "\n",
    "    one(occ_flags == 0, \"NON-OCCLUDING hair (occ=0)\")\n",
    "    one(occ_flags == 1, \"OCCLUDING hair (occ=1)\")\n",
    "\n",
    "    text = (title_prefix + \"\\n\" if title_prefix else \"\") + \"\\n\".join(lines)\n",
    "    print(text)\n",
    "\n",
    "    if out_path_txt is not None:\n",
    "        Path(out_path_txt).write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "#\n",
    "\n",
    "def run_one_seed(seed: int):\n",
    "    seed_everything(seed)\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "\n",
    "    run_dir = RUNS_DIR / f\"seed_{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_path = run_dir / \"best_model.pt\"\n",
    "    err_dir = run_dir / \"errors\"\n",
    "    err_fp = err_dir / \"female_as_male\"\n",
    "    err_fm = err_dir / \"male_as_female\"\n",
    "    err_fp.mkdir(parents=True, exist_ok=True)\n",
    "    err_fm.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(df, groups=df[\"museum_number\"]))\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n=== SEED {seed} ===\")\n",
    "    print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
    "    print(\"Val occ counts:\\n\", val_df[\"occluding_hair\"].value_counts())\n",
    "\n",
    "    train_ds = MultiTaskBboxDataset(train_df, transform=transform_train, pad=PAD)\n",
    "    val_ds = MultiTaskBboxDataset(val_df, transform=transform_val, pad=PAD)\n",
    "\n",
    "    train_gender = train_df[\"gender\"].map({\"female\": 0, \"male\": 1}).astype(int).to_numpy()\n",
    "    class_counts = np.bincount(train_gender, minlength=2).astype(float)\n",
    "    class_w = 1.0 / np.maximum(class_counts, 1.0)\n",
    "    sample_w = class_w[train_gender]\n",
    "    sample_w = torch.tensor(sample_w, dtype=torch.double)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_w,\n",
    "        num_samples=len(sample_w),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_keep_meta,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=gen,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_keep_meta,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=gen,\n",
    "    )\n",
    "\n",
    "    n_female = int((train_df[\"gender\"] == \"female\").sum())\n",
    "    n_male = int((train_df[\"gender\"] == \"male\").sum())\n",
    "    w_female = n_male / (n_female + n_male)\n",
    "    w_male = n_female / (n_female + n_male)\n",
    "    gender_weights = torch.tensor([w_female, w_male], dtype=torch.float32).to(device)\n",
    "    loss_gender = nn.CrossEntropyLoss(weight=gender_weights)\n",
    "\n",
    "    n_pos = int(train_df[\"occluding_hair\"].sum())\n",
    "    n_neg = int(len(train_df) - n_pos)\n",
    "    pos_weight = torch.tensor([n_neg / max(1, n_pos)], dtype=torch.float32).to(device)\n",
    "    loss_occ = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    in_features = backbone.fc.in_features\n",
    "    backbone.fc = nn.Identity()\n",
    "\n",
    "    head_gender = nn.Linear(in_features, 2)\n",
    "    head_occ = nn.Linear(in_features, 1)\n",
    "\n",
    "    class MultiHead(nn.Module):\n",
    "        def __init__(self, backbone, head_gender, head_occ):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            self.head_gender = head_gender\n",
    "            self.head_occ = head_occ\n",
    "\n",
    "        def forward(self, x):\n",
    "            feats = self.backbone(x)\n",
    "            lg = self.head_gender(feats)\n",
    "            lo = self.head_occ(feats).squeeze(1)\n",
    "            return lg, lo\n",
    "\n",
    "    model = MultiHead(backbone, head_gender, head_occ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, yg, yo, _ in tqdm(train_loader, desc=\"train\", leave=False):\n",
    "            x = x.to(device)\n",
    "            yg = yg.to(device)\n",
    "            yo = yo.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits_g, logits_o = model(x)\n",
    "\n",
    "            lg = loss_gender(logits_g, yg)\n",
    "            lo = loss_occ(logits_o, yo)\n",
    "            loss = lg + LAMBDA_OCC * lo\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += loss.item() * x.size(0)\n",
    "\n",
    "        return total / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_epoch(save_errors=False):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        occ_flags = []\n",
    "        occ_true, occ_pred = [], []\n",
    "\n",
    "        for x, yg, yo, metas in tqdm(val_loader, desc=\"val\", leave=False):\n",
    "            x = x.to(device)\n",
    "            logits_g, logits_o = model(x)\n",
    "\n",
    "            probs_g = torch.softmax(logits_g, dim=1).cpu().numpy()\n",
    "            preds_g = probs_g.argmax(axis=1)\n",
    "\n",
    "            preds_o = (torch.sigmoid(logits_o).cpu().numpy() >= 0.5).astype(int)\n",
    "\n",
    "            for i in range(len(preds_g)):\n",
    "                true_g = int(yg[i].item())\n",
    "                pred_g = int(preds_g[i])\n",
    "                pmale = float(probs_g[i][1])\n",
    "                meta = metas[i]\n",
    "\n",
    "                y_true.append(true_g)\n",
    "                y_pred.append(pred_g)\n",
    "                occ_flags.append(int(meta[\"occluding_hair\"]))\n",
    "\n",
    "                occ_true.append(int(yo[i].item()))\n",
    "                occ_pred.append(int(preds_o[i]))\n",
    "\n",
    "                if save_errors and true_g != pred_g:\n",
    "                    if true_g == 0 and pred_g == 1:\n",
    "                        save_error_crop(meta, pred_g, pmale, err_fp)\n",
    "                    elif true_g == 1 and pred_g == 0:\n",
    "                        save_error_crop(meta, pred_g, pmale, err_fm)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        bal = balanced_accuracy_score(y_true, y_pred)\n",
    "        f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        female_recall = cm[0, 0] / max(1, cm[0, 0] + cm[0, 1])\n",
    "\n",
    "        occ_acc = accuracy_score(occ_true, occ_pred)\n",
    "        return acc, bal, f1m, female_recall, occ_acc, y_true, y_pred, occ_flags\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_epoch = 0\n",
    "    pat_left = PATIENCE\n",
    "    hist = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch()\n",
    "        acc, bal, f1m, fem_rec, occ_acc, y_true, y_pred, occ_flags = eval_epoch(save_errors=False)\n",
    "\n",
    "        hist.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(tr_loss),\n",
    "            \"val_acc\": float(acc),\n",
    "            \"val_bal_acc\": float(bal),\n",
    "            \"val_f1_macro\": float(f1m),\n",
    "            \"val_female_recall\": float(fem_rec),\n",
    "            \"val_occ_acc\": float(occ_acc),\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | loss={tr_loss:.4f} | \"\n",
    "            f\"acc={acc:.3f} | bal_acc={bal:.3f} | f1m={f1m:.3f} | fem_rec={fem_rec:.3f} | occ_acc={occ_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        if f1m > best_f1 + MIN_DELTA:\n",
    "            best_f1 = f1m\n",
    "            best_epoch = epoch\n",
    "            pat_left = PATIENCE\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            pat_left -= 1\n",
    "            if pat_left <= 0:\n",
    "                break\n",
    "\n",
    "    pd.DataFrame(hist).to_csv(run_dir / \"history.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    acc, bal, f1m, fem_rec, occ_acc, y_true, y_pred, occ_flags = eval_epoch(save_errors=True)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    rep = classification_report(y_true, y_pred, target_names=[\"female\", \"male\"], digits=3, zero_division=0)\n",
    "\n",
    "    out_txt = run_dir / \"report.txt\"\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"SEED={seed}\\n\")\n",
    "        f.write(f\"BEST_EPOCH={best_epoch}\\n\")\n",
    "        f.write(f\"val_acc={acc:.3f}\\n\")\n",
    "        f.write(f\"val_bal_acc={bal:.3f}\\n\")\n",
    "        f.write(f\"val_macro_f1={f1m:.3f}\\n\")\n",
    "        f.write(f\"val_female_recall={fem_rec:.3f}\\n\")\n",
    "        f.write(f\"val_occ_acc={occ_acc:.3f}\\n\\n\")\n",
    "        f.write(\"CONFUSION MATRIX (gender)\\n\")\n",
    "        f.write(str(cm) + \"\\n\\n\")\n",
    "        f.write(rep + \"\\n\")\n",
    "\n",
    "    subgroup_metrics(\n",
    "        y_true, y_pred, occ_flags,\n",
    "        out_path_txt=run_dir / \"subgroup_report.txt\",\n",
    "        title_prefix=f\"SEED {seed} | BEST_EPOCH {best_epoch} | macroF1={f1m:.3f}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"val_acc\": acc,\n",
    "        \"val_bal_acc\": bal,\n",
    "        \"val_f1_macro\": f1m,\n",
    "        \"val_female_recall\": fem_rec,\n",
    "        \"val_occ_acc\": occ_acc,\n",
    "        \"val_occ_n\": int(np.sum(np.array(occ_flags) == 1)),\n",
    "        \"val_total\": len(occ_flags),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "#\n",
    "\n",
    "all_res = []\n",
    "for s in SEEDS:\n",
    "    all_res.append(run_one_seed(s))\n",
    "\n",
    "res_df = pd.DataFrame(all_res)\n",
    "res_df.to_csv(RUNS_DIR / \"summary_runs.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== SUMMARY (3 seeds) ===\")\n",
    "print(res_df[[\"seed\",\"best_epoch\",\"val_acc\",\"val_bal_acc\",\"val_f1_macro\",\"val_female_recall\",\"val_occ_acc\",\"val_occ_n\",\"val_total\"]])\n",
    "\n",
    "print(\"\\nAverages:\")\n",
    "print(res_df[[\"val_acc\",\"val_bal_acc\",\"val_f1_macro\",\"val_female_recall\",\"val_occ_acc\"]].mean(numeric_only=True))\n",
    "\n",
    "print(\"\\nStd:\")\n",
    "print(res_df[[\"val_acc\",\"val_bal_acc\",\"val_f1_macro\",\"val_female_recall\",\"val_occ_acc\"]].std(numeric_only=True))\n",
    "\n",
    "print(\"\\nOutputs in:\", RUNS_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
