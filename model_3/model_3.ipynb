{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce3486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Missing images: 0\n",
      "Total bboxes (male/female): 520\n",
      "Gender counts:\n",
      " gender\n",
      "male      359\n",
      "female    161\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: 425 Val: 95\n",
      "Train gender:\n",
      " gender\n",
      "male      296\n",
      "female    129\n",
      "Name: count, dtype: int64\n",
      "Val gender:\n",
      " gender\n",
      "male      63\n",
      "female    32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ready loaders: 425 95\n",
      "Train counts: {'female': 129, 'male': 296}\n",
      "Class weights: [0.69647056 0.3035294 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.6127 | val_acc=0.747 | val_f1_macro=0.721\n",
      "  New best macro-F1=0.721 (epoch 1) saved to gender_run_out_perspective_aug_A\\best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.2815 | val_acc=0.695 | val_f1_macro=0.681\n",
      "  no improvement, patience left: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.2019 | val_acc=0.768 | val_f1_macro=0.664\n",
      "  no improvement, patience left: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.0917 | val_acc=0.747 | val_f1_macro=0.721\n",
      "  no improvement, patience left: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.0746 | val_acc=0.779 | val_f1_macro=0.746\n",
      "  New best macro-F1=0.746 (epoch 5) saved to gender_run_out_perspective_aug_A\\best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.0942 | val_acc=0.768 | val_f1_macro=0.709\n",
      "  no improvement, patience left: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | loss=0.1064 | val_acc=0.758 | val_f1_macro=0.727\n",
      "  no improvement, patience left: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.0873 | val_acc=0.768 | val_f1_macro=0.741\n",
      "  no improvement, patience left: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | loss=0.1113 | val_acc=0.789 | val_f1_macro=0.756\n",
      "  New best macro-F1=0.756 (epoch 9) saved to gender_run_out_perspective_aug_A\\best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | loss=0.0967 | val_acc=0.726 | val_f1_macro=0.712\n",
      "  no improvement, patience left: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | loss=0.0538 | val_acc=0.832 | val_f1_macro=0.783\n",
      "  New best macro-F1=0.783 (epoch 11) saved to gender_run_out_perspective_aug_A\\best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | loss=0.0602 | val_acc=0.832 | val_f1_macro=0.793\n",
      "  New best macro-F1=0.793 (epoch 12) saved to gender_run_out_perspective_aug_A\\best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | loss=0.0199 | val_acc=0.789 | val_f1_macro=0.764\n",
      "  no improvement, patience left: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | loss=0.0426 | val_acc=0.779 | val_f1_macro=0.758\n",
      "  no improvement, patience left: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | loss=0.0322 | val_acc=0.811 | val_f1_macro=0.777\n",
      "  no improvement, patience left: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | loss=0.0295 | val_acc=0.821 | val_f1_macro=0.791\n",
      "  no improvement, patience left: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | loss=0.0273 | val_acc=0.779 | val_f1_macro=0.754\n",
      "  no improvement, patience left: 0/5\n",
      "\n",
      "Early stopping at epoch 17. Best macro-F1=0.793 at epoch 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFUSION MATRIX (rows=true, cols=pred):\n",
      " [[19 13]\n",
      " [ 3 60]]\n",
      "\n",
      "REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      female      0.864     0.594     0.704        32\n",
      "        male      0.822     0.952     0.882        63\n",
      "\n",
      "    accuracy                          0.832        95\n",
      "   macro avg      0.843     0.773     0.793        95\n",
      "weighted avg      0.836     0.832     0.822        95\n",
      "\n",
      "\n",
      "Best model: gender_run_out_perspective_aug_A\\best_model.pt | best val_f1_macro=0.793 at epoch 12\n",
      "All outputs in: C:\\Users\\Katya\\mag_vase\\ML\\gender_run_out_perspective_aug_A\n",
      "Mistake crops: C:\\Users\\Katya\\mag_vase\\ML\\gender_run_out_perspective_aug_A\\mistakes\n",
      "Low-confidence crops: C:\\Users\\Katya\\mag_vase\\ML\\gender_run_out_perspective_aug_A\\low_confidence\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "JSON_PATH = Path(\"LS_export_26.12.json\")\n",
    "LABELS_CSV = Path(\"label_type_gender.csv\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "PATIENCE = 5\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "PAD = 0.0\n",
    "MIN_CROP_PX = 16\n",
    "\n",
    "OUT_DIR = Path(\"gender_run_out_perspective_aug_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH = OUT_DIR / \"best_model.pt\"\n",
    "\n",
    "MIS_DIR = OUT_DIR / \"mistakes\"\n",
    "LOW_DIR = OUT_DIR / \"low_confidence\"\n",
    "MIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_TOP_MISTAKES = 60\n",
    "SAVE_TOP_LOWCONF = 60\n",
    "LOWCONF_THRESHOLD = 0.55\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = (torch.initial_seed() + worker_id) % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# \n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "label_df = pd.read_csv(LABELS_CSV, encoding=\"utf-8\")\n",
    "label2gender = dict(zip(label_df[\"label\"], label_df[\"gender\"]))\n",
    "\n",
    "rows = []\n",
    "missing_images = 0\n",
    "\n",
    "for task in tasks:\n",
    "    task_id = task.get(\"id\")\n",
    "    data = task.get(\"data\", {}) or {}\n",
    "\n",
    "    image_path = data.get(\"image_local_path\")\n",
    "    museum_number = data.get(\"Museum number\")\n",
    "\n",
    "    if not image_path:\n",
    "        missing_images += 1\n",
    "        continue\n",
    "\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.exists():\n",
    "        missing_images += 1\n",
    "        continue\n",
    "\n",
    "    for ann in task.get(\"annotations\", []):\n",
    "        for r in ann.get(\"result\", []):\n",
    "            if r.get(\"type\") != \"rectanglelabels\":\n",
    "                continue\n",
    "\n",
    "            v = r.get(\"value\", {}) or {}\n",
    "            labels = v.get(\"rectanglelabels\", [])\n",
    "            if not labels:\n",
    "                continue\n",
    "\n",
    "            label = labels[0]\n",
    "            gender = label2gender.get(label)\n",
    "            if gender not in {\"male\", \"female\"}:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"museum_number\": museum_number,\n",
    "                \"image_path\": str(image_path),\n",
    "                \"label\": label,\n",
    "                \"gender\": gender,\n",
    "                \"x\": float(v[\"x\"]),\n",
    "                \"y\": float(v[\"y\"]),\n",
    "                \"w\": float(v[\"width\"]),\n",
    "                \"h\": float(v[\"height\"]),\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Missing images:\", missing_images)\n",
    "print(\"Total bboxes (male/female):\", len(df))\n",
    "print(\"Gender counts:\\n\", df[\"gender\"].value_counts(dropna=False))\n",
    "\n",
    "df.to_csv(OUT_DIR / \"bboxes_gender_df.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# \n",
    "\n",
    "df[\"museum_number\"] = df[\"museum_number\"].fillna(df[\"task_id\"].astype(str))\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"museum_number\"]))\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTrain:\", len(train_df), \"Val:\", len(val_df))\n",
    "print(\"Train gender:\\n\", train_df[\"gender\"].value_counts())\n",
    "print(\"Val gender:\\n\", val_df[\"gender\"].value_counts())\n",
    "\n",
    "train_df.to_csv(OUT_DIR / \"train_df.csv\", index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(OUT_DIR / \"val_df.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "#\n",
    "\n",
    "class GenderBboxDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, pad: float = 0.0):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.pad = pad\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.image_path).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "\n",
    "        x1 = row.x / 100.0 * W\n",
    "        y1 = row.y / 100.0 * H\n",
    "        x2 = (row.x + row.w) / 100.0 * W\n",
    "        y2 = (row.y + row.h) / 100.0 * H\n",
    "\n",
    "        if (x2 - x1) < MIN_CROP_PX or (y2 - y1) < MIN_CROP_PX:\n",
    "            raise ValueError(\n",
    "                f\"Too-small crop at idx={idx}: \"\n",
    "                f\"w={(x2-x1):.2f}px h={(y2-y1):.2f}px \"\n",
    "                f\"task_id={row.task_id} image={row.image_path}\"\n",
    "            )\n",
    "\n",
    "        if self.pad > 0:\n",
    "            pad_x = self.pad * (x2 - x1)\n",
    "            pad_y = self.pad * (y2 - y1)\n",
    "            x1 = max(0, x1 - pad_x)\n",
    "            y1 = max(0, y1 - pad_y)\n",
    "            x2 = min(W, x2 + pad_x)\n",
    "            y2 = min(H, y2 + pad_y)\n",
    "\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        y = 1 if row.gender == \"male\" else 0\n",
    "\n",
    "        meta = {\n",
    "            \"task_id\": row.task_id,\n",
    "            \"museum_number\": row.museum_number,\n",
    "            \"image_path\": row.image_path,\n",
    "            \"label\": row.label,\n",
    "            \"gender\": row.gender,\n",
    "            \"bbox_px\": (float(x1), float(y1), float(x2), float(y2)),\n",
    "        }\n",
    "        return crop, y, meta\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    xs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    ys = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    metas = [b[2] for b in batch]\n",
    "    return xs, ys, metas\n",
    "\n",
    "# \n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "\n",
    "    T.RandomPerspective(distortion_scale=0.10, p=0.25),\n",
    "\n",
    "    T.RandomAffine(\n",
    "        degrees=4,\n",
    "        translate=(0.02, 0.02),\n",
    "        scale=(0.98, 1.02),\n",
    "        shear=None,\n",
    "    ),\n",
    "\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "    T.RandomErasing(p=0.05, scale=(0.01, 0.03), ratio=(0.3, 3.3), value=\"random\"),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#\n",
    "\n",
    "train_ds = GenderBboxDataset(train_df, transform=transform_train, pad=PAD)\n",
    "val_ds = GenderBboxDataset(val_df, transform=transform_val, pad=PAD)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_keep_meta,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_keep_meta,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "print(\"\\nReady loaders:\", len(train_ds), len(val_ds))\n",
    "\n",
    "#\n",
    "\n",
    "n_female = int((train_df[\"gender\"] == \"female\").sum())\n",
    "n_male = int((train_df[\"gender\"] == \"male\").sum())\n",
    "print(\"Train counts:\", {\"female\": n_female, \"male\": n_male})\n",
    "\n",
    "w_female = n_male / (n_female + n_male)  # class 0\n",
    "w_male = n_female / (n_female + n_male)  # class 1\n",
    "class_weights = torch.tensor([w_female, w_male], dtype=torch.float32).to(device)\n",
    "print(\"Class weights:\", class_weights.detach().cpu().numpy())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "#\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "#\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y, _metas in tqdm(train_loader, desc=\"train\", leave=False):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def _save_crop_with_caption(meta, pred_label, p_male, out_path: Path):\n",
    "    img = Image.open(meta[\"image_path\"]).convert(\"RGB\")\n",
    "    x1, y1, x2, y2 = meta[\"bbox_px\"]\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    conf = max(p_male, 1.0 - p_male)\n",
    "    caption = f\"true={meta['gender']} | pred={pred_label} | P(male)={p_male:.3f} | conf={conf:.3f} | label={meta['label']}\"\n",
    "\n",
    "    W, H = crop.size\n",
    "    canvas = Image.new(\"RGB\", (W, H + 40), (255, 255, 255))\n",
    "    canvas.paste(crop, (0, 0))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    draw.text((6, H + 10), caption, fill=(0, 0, 0), font=font)\n",
    "    canvas.save(out_path)\n",
    "\n",
    "def save_diagnostics(y_true, y_pred, p_male_list, metas):\n",
    "    mistakes = []\n",
    "    lowconf = []\n",
    "\n",
    "    for yt, yp, pm, meta in zip(y_true, y_pred, p_male_list, metas):\n",
    "        true_label = \"male\" if yt == 1 else \"female\"\n",
    "        pred_label = \"male\" if yp == 1 else \"female\"\n",
    "        conf = max(pm, 1.0 - pm)\n",
    "\n",
    "        if true_label != pred_label:\n",
    "            mistakes.append((conf, meta, pred_label, pm))\n",
    "\n",
    "        if conf < LOWCONF_THRESHOLD:\n",
    "            lowconf.append((conf, meta, pred_label, pm))\n",
    "\n",
    "    mistakes.sort(key=lambda x: x[0], reverse=True)\n",
    "    lowconf.sort(key=lambda x: x[0])\n",
    "\n",
    "    for i, (_conf, meta, pred_label, pm) in enumerate(mistakes[:SAVE_TOP_MISTAKES], start=1):\n",
    "        out = MIS_DIR / f\"mistake_{i:03d}_task{meta['task_id']}.jpg\"\n",
    "        _save_crop_with_caption(meta, pred_label, pm, out)\n",
    "\n",
    "    for i, (_conf, meta, pred_label, pm) in enumerate(lowconf[:SAVE_TOP_LOWCONF], start=1):\n",
    "        out = LOW_DIR / f\"lowconf_{i:03d}_task{meta['task_id']}.jpg\"\n",
    "        _save_crop_with_caption(meta, pred_label, pm, out)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(save_examples: bool = False):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    p_male_list = []\n",
    "    metas_all = []\n",
    "\n",
    "    for x, y, metas in tqdm(val_loader, desc=\"val\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        p = torch.softmax(logits, dim=1).cpu()\n",
    "\n",
    "        pred = p.argmax(dim=1).numpy().tolist()\n",
    "        p_male = p[:, 1].numpy().tolist()\n",
    "\n",
    "        y_true += y.numpy().tolist()\n",
    "        y_pred += pred\n",
    "        p_male_list += p_male\n",
    "        metas_all += metas\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    if save_examples:\n",
    "        save_diagnostics(y_true, y_pred, p_male_list, metas_all)\n",
    "\n",
    "    return acc, f1m, y_true, y_pred\n",
    "\n",
    "#\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_epoch = 0\n",
    "pat_left = PATIENCE\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch()\n",
    "    acc, f1m, y_true, y_pred = eval_epoch(save_examples=False)\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(train_loss),\n",
    "        \"val_acc\": float(acc),\n",
    "        \"val_f1_macro\": float(f1m),\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | loss={train_loss:.4f} | val_acc={acc:.3f} | val_f1_macro={f1m:.3f}\")\n",
    "\n",
    "    if f1m > best_f1 + MIN_DELTA:\n",
    "        best_f1 = f1m\n",
    "        best_epoch = epoch\n",
    "        pat_left = PATIENCE\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"  New best macro-F1={best_f1:.3f} (epoch {best_epoch}) saved to {BEST_PATH}\")\n",
    "    else:\n",
    "        pat_left -= 1\n",
    "        print(f\"  no improvement, patience left: {pat_left}/{PATIENCE}\")\n",
    "        if pat_left <= 0:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}. Best macro-F1={best_f1:.3f} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "pd.DataFrame(history).to_csv(OUT_DIR / \"history.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "#\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_PATH, map_location=device))\n",
    "acc, f1m, y_true, y_pred = eval_epoch(save_examples=True)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "report = classification_report(y_true, y_pred, target_names=[\"female\", \"male\"], digits=3, zero_division=0)\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nREPORT:\\n\", report)\n",
    "\n",
    "with open(OUT_DIR / \"report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"CONFUSION MATRIX (rows=true, cols=pred)\\n\")\n",
    "    f.write(str(cm) + \"\\n\\n\")\n",
    "    f.write(report + \"\\n\")\n",
    "\n",
    "print(f\"\\nBest model: {BEST_PATH} | best val_f1_macro={best_f1:.3f} at epoch {best_epoch}\")\n",
    "print(f\"All outputs in: {OUT_DIR.resolve()}\")\n",
    "print(f\"Mistake crops: {MIS_DIR.resolve()}\")\n",
    "print(f\"Low-confidence crops: {LOW_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
